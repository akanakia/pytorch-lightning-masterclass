{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim, tensor, no_grad, randn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "mnist_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train=True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_split(dataset, train_fraction = 0.8):\n",
    "    \"\"\"\n",
    "    Returns the absolute value of items to split into (train, test) from the input dataset\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    dataset : The torchvision dataset to split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_fraction : Int (Default = 0.8)\n",
    "        The fraction of data points that will be used for training. Must be between [0, 1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_size, test_size : Tuple (int, int)\n",
    "        The absolute number of data points to in train vs test.\n",
    "    \"\"\"\n",
    "    size = dataset.data.size()[0]\n",
    "    train_size = int(size * train_fraction)\n",
    "    test_size = size - train_size\n",
    "    return train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "train_val_split = 0.8\n",
    "train, val = random_split(mnist_data, get_abs_split(mnist_data, train_val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the train and test data\n",
    "train_loader_batch_size = 32\n",
    "train_loader = DataLoader(train, batch_size = train_loader_batch_size)\n",
    "\n",
    "val_loader_batch_size = 32\n",
    "val_loader = DataLoader(val, batch_size = val_loader_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture parameters\n",
    "mnist_image_size = mnist_data.data.size()[1] *  mnist_data.data.size()[2]\n",
    "linear1_dims = 64\n",
    "linear2_dims = 64\n",
    "output_dims = 10 # predicting 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "\n",
    "# model = nn.Sequential(OrderedDict([\n",
    "#     ('linear1', nn.Linear(mnist_image_size, linear1_dims)),\n",
    "#     ('relu1', nn.ReLU()),\n",
    "#     ('linear2', nn.Linear(linear1_dims, linear2_dims)),\n",
    "#     ('relu2', nn.ReLU()),\n",
    "#     ('dropout', nn.Dropout(0.1)), # in case of overfitting\n",
    "#     ('output', nn.Linear(linear2_dims, output_dims))\n",
    "# ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more flexible model\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self, input_dims, linear1_dims, linear2_dims, output_dims, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, linear1_dims)\n",
    "        self.linear2 = nn.Linear(linear1_dims, linear2_dims)\n",
    "        self.linear3 = nn.Linear(linear2_dims, output_dims)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden1 = nn.functional.relu(self.linear1(x))\n",
    "        hidden2 = nn.functional.relu(self.linear2(hidden1))\n",
    "        dropout = self.dropout(hidden2 + hidden1) # Creates a residual connection\n",
    "        logits = self.linear3(dropout)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel(mnist_image_size, linear1_dims, linear2_dims, output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer\n",
    "lr = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 0.87\n",
      "Epoch 1, validation loss: 0.45\n",
      "Epoch 2, training loss: 0.39\n",
      "Epoch 2, validation loss: 0.37\n",
      "Epoch 3, training loss: 0.33\n",
      "Epoch 3, validation loss: 0.32\n",
      "Epoch 4, training loss: 0.29\n",
      "Epoch 4, validation loss: 0.28\n",
      "Epoch 5, training loss: 0.26\n",
      "Epoch 5, validation loss: 0.26\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    train_losses = list()\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # flatten x : batch size * num channels (= 1) * image size (= 28 * 28)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        logits = model(x)\n",
    "\n",
    "        # 2. Compute objective\n",
    "        J = loss(logits, y)\n",
    "\n",
    "        # 3. Clean the gradients. We are updating gradients per batch and not accumulating so clear them between each batch.\n",
    "        model.zero_grad() # Note: Can also zero gradients from the optimizer. optimizer.zero_grad()\n",
    "\n",
    "        # 4. Backward pass. Accumulate the partial derivatives.\n",
    "        J.backward()\n",
    "\n",
    "        # 5. Learn and update weights and biases\n",
    "        optimizer.step()\n",
    "        train_losses.append(J.item())\n",
    "    print(f'Epoch {epoch + 1}, training loss: {tensor(train_losses).mean():.2f}')\n",
    "\n",
    "    # Validation loop\n",
    "    val_losses = list()\n",
    "    for batch in val_loader:\n",
    "        x, y = batch\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # flatten x : batch size * num channels (= 1) * image size (= 28 * 28)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        with no_grad():\n",
    "            logits = model(x)\n",
    "\n",
    "        # 2. Compute objective\n",
    "        J = loss(logits, y)\n",
    "        val_losses.append(J.item())\n",
    "    print(f'Epoch {epoch + 1}, validation loss: {tensor(val_losses).mean():.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('pytorch-lightning-masterclass-qFiovNJT-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aedc27f9552936948c912cef8aa231682fec7a8ec2ec30d6ca6884933c505a53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
