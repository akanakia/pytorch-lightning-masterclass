{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim, tensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch lightning class\n",
    "class LinearImageClassifier(pl.LightningModule):\n",
    "    @staticmethod\n",
    "    def _get_abs_split(dataset, train_fraction):\n",
    "        \"\"\"\n",
    "        Returns the absolute value of items to split into (train, test) from the input dataset\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        dataset : The torchvision dataset to split.\n",
    "\n",
    "        train_fraction : Float\n",
    "            The fraction of data points that will be used for training. Must be between [0, 1].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_size, test_size : Tuple (int, int)\n",
    "            The absolute number of data points to in train vs test.\n",
    "        \"\"\"\n",
    "        size = dataset.data.size()[0]\n",
    "        train_size = int(size * train_fraction)\n",
    "        test_size = size - train_size\n",
    "        return train_size, test_size\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            training_data,\n",
    "            linear_dims,\n",
    "            train_fraction = 0.8,\n",
    "            dropout_rate = 0.1,\n",
    "            create_residual_connection = False,\n",
    "            learning_rate = 1e-2,\n",
    "            batch_size = 10):\n",
    "        \"\"\"\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        training_data : TorchVision dataset or similar\n",
    "\n",
    "        linear_dims : List[int]\n",
    "            input/output dimensions of linear units in sequence.\n",
    "            E.g., for a 2 layer lienear network it would be \n",
    "            [ input_dims, linear1_dims, linear2_dims, output_dims ]\n",
    "            Note: the current implementation does not support more than 5 hidden linear layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dropout_rate : Float (Default = 0)\n",
    "            Can be set to a value between [0, 1] to add a final dropout layer.\n",
    "\n",
    "        create_residual_connection : Bool (Default = False)\n",
    "            Will create a residual connection in the dropout layer between the first and last hidden layers.\n",
    "\n",
    "        learning_rate : Float (Default = 1e-2)\n",
    "\n",
    "        batch_size : Int (Default = 10)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Input and hidden layers\n",
    "        self._layers = nn.ModuleList()\n",
    "        for i in range(len(linear_dims) - 2):\n",
    "            self._layers.append(nn.Linear(linear_dims[i], linear_dims[i + 1]))\n",
    "\n",
    "        # Dropout layer, if any\n",
    "        self._dropout_rate = dropout_rate\n",
    "        if self._dropout_rate > 0:\n",
    "            self._layers.append(nn.Dropout(self._dropout_rate))\n",
    "        self._create_residual_connection = create_residual_connection\n",
    "\n",
    "        # Output layer\n",
    "        self._layers.append(nn.Linear(linear_dims[-2], linear_dims[-1]))\n",
    "\n",
    "        self._training_data = training_data\n",
    "        self._train_fraction = train_fraction\n",
    "        self._total_hidden_layers = len(linear_dims) - 2\n",
    "        self._learning_rate = learning_rate\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Can be used to download data but do not assign anything to the model here\n",
    "        # i.e., no self.something = value here because this method will run only on a single processor.\n",
    "        # To initialize, transform and assign data use the setup() method instead.\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # This method will run in every GPU and logical processor so use it to assign model parameters.\n",
    "        # Split the data into train and test\n",
    "        self._training_data, self._validation_data = random_split(self._training_data, self._get_abs_split(self._training_data, self._train_fraction))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Create data loaders for the train and test data\n",
    "        train_loader = DataLoader(self._training_data, batch_size = self._batch_size, num_workers = os.cpu_count())\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Create data loaders for the train and test data\n",
    "        val_loader = DataLoader(self._validation_data, batch_size = self._batch_size, num_workers = os.cpu_count())\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define an optimizer\n",
    "        optimizer = optim.SGD(self.parameters(), lr = self._learning_rate)\n",
    "        return [ optimizer ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        forward_pass = list()\n",
    "        forward_pass.append(nn.functional.relu(self._layers[0](x)))\n",
    "\n",
    "        if self._total_hidden_layers > 0:\n",
    "            forward_pass.append(nn.functional.relu(self._layers[1](forward_pass[0])))\n",
    "        if self._total_hidden_layers > 1:\n",
    "            forward_pass.append(nn.functional.relu(self._layers[2](forward_pass[1])))\n",
    "        if self._total_hidden_layers > 2:\n",
    "            forward_pass.append(nn.functional.relu(self._layers[3](forward_pass[2])))\n",
    "        if self._total_hidden_layers > 3:\n",
    "            forward_pass.append(nn.functional.relu(self._layers[4](forward_pass[3])))\n",
    "        if self._total_hidden_layers > 4:\n",
    "            forward_pass.append(nn.functional.relu(self._layers[5](forward_pass[4])))\n",
    "        # Only 5 hidden layers are currently supported \n",
    "\n",
    "        if self._dropout_rate > 0:\n",
    "            if self._create_residual_connection:\n",
    "                forward_pass.append(self._layers[-2](forward_pass[-1] + forward_pass[0]))\n",
    "            else:\n",
    "                forward_pass.append(self._layers[-2](forward_pass[-1]))\n",
    "\n",
    "        # Get the logits from the output layer\n",
    "        logits = self._layers[-1](forward_pass[-1])\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_i):\n",
    "        # Get batch\n",
    "        x, y = batch\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # flatten x : batch size * num channels (= 1) * image size (= 28 * 28)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # 2. Compute objective and other metrics\n",
    "        J = self._loss(logits, y)\n",
    "\n",
    "        acc = accuracy(logits, y)\n",
    "        progress_bar = { 'accuracy': acc }\n",
    "        return { 'loss': J, 'progress_bar': progress_bar } # 'loss' and 'progress_bar' are pytorch lightning keywords\n",
    "\n",
    "    def validation_step(self, batch, batch_i):\n",
    "        results = self.training_step(batch, batch_i)\n",
    "        return results\n",
    "\n",
    "    def validates_epoch_end(self, val_step_outputs):\n",
    "        avg_val_loss = tensor([ o['loss'] for o in val_step_outputs ]).mean()\n",
    "        avg_accuracy = tensor([ o['progress_bar']['accuracy'] for o in val_step_outputs ]).mean()\n",
    "        progress_bar = { 'val_accuracy': avg_accuracy }\n",
    "        return { 'val_loss': avg_val_loss, 'progress_bar': progress_bar } # 'val_loss' and 'progress_bar' are pytorch lightning keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture parameters\n",
    "mnist_image_size = mnist_data.data.size()[1] *  mnist_data.data.size()[2]\n",
    "linear1_dims = 64\n",
    "linear2_dims = 64\n",
    "output_dims = 10 # predicting 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearImageClassifier(\n",
    "    training_data = mnist_data,\n",
    "    linear_dims = [ mnist_image_size, linear1_dims, linear2_dims, output_dims ],\n",
    "    create_residual_connection = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs = 2)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('pytorch-lightning-masterclass-qFiovNJT-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aedc27f9552936948c912cef8aa231682fec7a8ec2ec30d6ca6884933c505a53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
